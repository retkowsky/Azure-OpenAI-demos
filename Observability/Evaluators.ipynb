{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c7ed3c-437d-4c87-b8b2-a12451040973",
   "metadata": {},
   "source": [
    "# General purpose and text similarity evaluators \n",
    "\n",
    "AI systems might generate textual responses that are incoherent, or lack the general writing quality you might desire beyond minimum grammatical correctness. To address these issues, use **Coherence and Fluency**.\n",
    "\n",
    "If you have a question-answering (QA) scenario with both context and ground truth data in addition to query and response, you can also use our **QAEvaluator** a composite evaluator that uses relevant evaluators for judgment.\n",
    "\n",
    "It's important to compare how closely the textual response generated by your AI system matches the response you would expect, typically called the \"ground truth\". Use LLM-judge metric like SimilarityEvaluator with a focus on the semantic similarity between the generated response and the ground truth, or use metrics from the field of natural language processing (NLP) including **F1 Score, BLEU, GLEU, ROUGE, and METEOR** with a focus on the overlaps of tokens or n-grams between the two.\n",
    "\n",
    "> https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/general-purpose-evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927c215f-a50a-45f2-8d7e-4f84d9cabe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0d82ef-21d6-4768-a1f0-e5802251d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration, BleuScoreEvaluator, CoherenceEvaluator, F1ScoreEvaluator, FluencyEvaluator, GleuScoreEvaluator, MeteorScoreEvaluator, QAEvaluator, RougeScoreEvaluator, RougeType, SimilarityEvaluator\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "969dae7d-8e84-49fe-8791-ff6db9aed1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf7833e-965f-4c42-874d-a50da21cb27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 26-Jun-2025 12:34:11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Today is {datetime.datetime.today().strftime('%d-%b-%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a217cb-cdc9-42cb-8f60-1d056f8c1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"azure.env\")\n",
    "\n",
    "endpoint = os.getenv(\"endpoint\")\n",
    "key = os.getenv(\"key\")\n",
    "\n",
    "azure_deployment = \"gpt-4.1\"\n",
    "api_version = \"2024-10-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76d2da8-6ca2-4e40-80a9-e3796198a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=key,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1820508-924a-4588-8e7b-9c8f2545641a",
   "metadata": {},
   "source": [
    "## Coherence evaluation\n",
    "\n",
    "> Measures logical consistency and flow of responses.\n",
    "\n",
    "CoherenceEvaluator measures the logical and orderly presentation of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought. A coherent response directly addresses the question with clear connections between sentences and paragraphs, using appropriate transitions and a logical sequence of ideas. Higher scores mean better coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25ec302-dc66-44be-82b8-d1e4bf64ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_evaluator = CoherenceEvaluator(model_config=model_config, threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d383aed-a90f-4104-a831-141535bfbaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coherence': 4.0,\n",
       " 'gpt_coherence': 4.0,\n",
       " 'coherence_reason': 'The response is clear, logically organized, and directly answers the question, but its simplicity does not demonstrate advanced coherence.',\n",
       " 'coherence_result': 'pass',\n",
       " 'coherence_threshold': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_evaluator(query=\"What is the capital of France?\", response=\"The capital of France is Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e356f0e1-64a1-4a1d-9f78-d29364036b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coherence': 2.0,\n",
       " 'gpt_coherence': 2.0,\n",
       " 'coherence_reason': 'The response is a fragmented answer with some relevant words but lacks logical structure and does not address the question directly.',\n",
       " 'coherence_result': 'fail',\n",
       " 'coherence_threshold': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_evaluator(query=\"Is Marie Curie is born in Paris?\", response=\"She is living in Paris.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328335d-3870-4271-b5b5-9669e4b76d51",
   "metadata": {},
   "source": [
    "## Fluency evaluator\n",
    "\n",
    "> Measures natural language quality and readability.\n",
    "\n",
    "FluencyEvaluatormeasures the effectiveness and clarity of written communication, focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, and overall readability. It assesses how smoothly ideas are conveyed and how easily the reader can understand the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e978990-3d20-428f-9a7f-0e2f84d277f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency_evaluator = FluencyEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e348924b-752a-49c3-8594-32cced9d2e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fluency': 3.0,\n",
       " 'gpt_fluency': 3.0,\n",
       " 'fluency_reason': 'The response is clear and correct, but uses basic vocabulary and a simple sentence structure without complexity or variety.',\n",
       " 'fluency_result': 'pass',\n",
       " 'fluency_threshold': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluency_evaluator(response=\"Victor Hugo is a writer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97e2ade-1ddd-4a5a-9a52-b2f7b2a0a389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fluency': 5.0,\n",
       " 'gpt_fluency': 5.0,\n",
       " 'fluency_reason': 'The response is well-articulated, uses varied vocabulary, and is grammatically flawless. It is coherent and cohesive, reflecting a high level of fluency.',\n",
       " 'fluency_result': 'pass',\n",
       " 'fluency_threshold': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluency_evaluator(response=\"Victor Hugo was a renowned French Romantic writer, best known for his novels 'Les Misérables' and 'Notre-Dame de Paris'. He was also a poet, dramatist, and a significant political figure in France.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80d809-6b12-46fb-b18f-45709a419047",
   "metadata": {},
   "source": [
    "## QA Evaluator\n",
    "\n",
    "> Measures comprehensively various quality aspects in question-answering.\n",
    "\n",
    "QAEvaluator measures comprehensively various aspects in a question-answering scenario:\n",
    "- Relevance\n",
    "- Groundedness\n",
    "- Fluency\n",
    "- Coherence\n",
    "- Similarity\n",
    "- F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3acda1e-5f8e-454f-a9ee-9278438da61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_evaluator = QAEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ec09ad-3e49-4d4e-aa7f-584343052d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.631578947368421,\n",
       " 'f1_result': 'pass',\n",
       " 'f1_threshold': 3,\n",
       " 'similarity': 5.0,\n",
       " 'gpt_similarity': 5.0,\n",
       " 'similarity_result': 'pass',\n",
       " 'similarity_threshold': 3,\n",
       " 'fluency': 3.0,\n",
       " 'gpt_fluency': 3.0,\n",
       " 'fluency_reason': 'The response is clear and correct, with adequate vocabulary and sentence structure, but it does not demonstrate advanced fluency or complexity.',\n",
       " 'fluency_result': 'pass',\n",
       " 'fluency_threshold': 3,\n",
       " 'groundedness': 3.0,\n",
       " 'gpt_groundedness': 3.0,\n",
       " 'groundedness_reason': 'The response provides correct information, but it is not grounded in the provided context, as the context does not mention Warsaw or her place of birth at all.',\n",
       " 'groundedness_result': 'pass',\n",
       " 'groundedness_threshold': 3,\n",
       " 'relevance': 4.0,\n",
       " 'gpt_relevance': 4.0,\n",
       " 'relevance_reason': 'The response fully and accurately answers the question by stating Marie Curie was born in Warsaw, which is all the essential information required.',\n",
       " 'relevance_result': 'pass',\n",
       " 'relevance_threshold': 3,\n",
       " 'coherence': 4.0,\n",
       " 'gpt_coherence': 4.0,\n",
       " 'coherence_reason': 'The response is coherent, directly answers the question, and presents information in a logical and clear manner, though it is brief.',\n",
       " 'coherence_result': 'pass',\n",
       " 'coherence_threshold': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_evaluator(\n",
    "    query=\"Where was Marie Curie born?\",\n",
    "    context=\n",
    "    \"Background: 1. Marie Curie was a chemist. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist.\",\n",
    "    response=\n",
    "    \"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c66c2-2ddc-49be-a0ff-5068c2fd2e91",
   "metadata": {},
   "source": [
    "## Similarity Evaluator\n",
    "\n",
    "> AI-assisted textual similarity measurement.\n",
    "\n",
    "SimilarityEvaluator measures the degrees of semantic similarity between the generated text and its ground truth with respect to a query. Compared to other text-similarity metrics that require ground truths, this metric focuses on semantics of a response (instead of simple overlap in tokens or n-grams) and also considers the broader context of a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16da04da-4093-44b7-89b1-8d6519256c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_evaluator = SimilarityEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ffade5-19e0-45af-8141-8f6dbb90c31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity': 5.0, 'gpt_similarity': 5.0, 'similarity_result': 'pass', 'similarity_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "response = similarity_evaluator(\n",
    "    query=\"How many atoms in water?\",\n",
    "    response=\n",
    "    \"Water, chemically known as H₂O, is composed of two hydrogen atoms and one oxygen atom. Therefore, a single molecule of water contains a total of three atoms.\",\n",
    "    ground_truth=\"Three atoms: two hydrogen atoms and one oxygen atom\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841e0a31-e4f4-4b73-b975-19aacf9586b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity': 2.0, 'gpt_similarity': 2.0, 'similarity_result': 'fail', 'similarity_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "response = similarity_evaluator(query=\"How many atoms in water?\",\n",
    "           response=\"Water is composed of three hydrogen atoms.\",\n",
    "           ground_truth=\"Three atoms: two hydrogen atoms and one oxygen atom\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1716c-5540-4871-b291-842b3fc6bd76",
   "metadata": {},
   "source": [
    "## F1 score\n",
    "\n",
    "> Harmonic mean of precision and recall in token overlaps between response and ground truth.\n",
    "\n",
    "F1ScoreEvaluator measures the similarity by shared tokens between the generated text and the ground truth, focusing on both precision and recall. The F1-score computes the ratio of the number of shared words between the model generation and the ground truth. Ratio is computed over the individual words in the generated response against those in the ground truth answer. The number of shared words between the generation and the truth is the basis of the F1 score. Precision is the ratio of the number of shared words to the total number of words in the generation. Recall is the ratio of the number of shared words to the total number of words in the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4b0cede-97a2-489b-bdde-37c019ff9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_evaluator = F1ScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d7225ec-9f83-4565-94c0-0ce3087816ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28571428571428575\n"
     ]
    }
   ],
   "source": [
    "answer = f1_evaluator(response=\"I am walking in the street\", ground_truth=\"Just walking\")\n",
    "\n",
    "print(answer[\"f1_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa37c6-a86b-4042-a79f-47e59a4c09ef",
   "metadata": {},
   "source": [
    "## BLEU score\n",
    "\n",
    "> Bilingual Evaluation Understudy score for translation quality measures overlaps in n-grams between response and ground truth.\n",
    "\n",
    "BleuScoreEvaluator computes the BLEU (Bilingual Evaluation Understudy) score commonly used in natural language processing (NLP) and machine translation. It measures how closely the generated text matches the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cbfb6d0-da71-4532-a234-08a5492a0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = BleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fec7b65-ab8a-46e3-aa32-d854ff9ad6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "answer = bleu_score(response=\"I am walking in the street\",\n",
    "                    ground_truth=\"I am walking in a street\")\n",
    "\n",
    "print(answer[\"bleu_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8ac5d-6813-478c-9aab-caec7cdbe08f",
   "metadata": {},
   "source": [
    "## Gleu Score\n",
    "\n",
    "> Google-BLEU variant for sentence-level assessment measures overlaps in n-grams between response and ground truth.\n",
    "\n",
    "GleuScoreEvaluator computes the GLEU (Google-BLEU) score. It measures the similarity by shared n-grams between the generated text and ground truth, similar to the BLEU score, focusing on both precision and recall. But it addresses the drawbacks of the BLEU score using a per-sentence reward objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "658113eb-f645-4c09-93f3-c4bb6c38c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_score = GleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf58c355-cccf-443d-a531-e4425436b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "answer = gleu_score(response=\"I am walking in the street\",\n",
    "                    ground_truth=\"I am walking in a street\")\n",
    "\n",
    "print(answer[\"gleu_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814d2fe-3ca6-4c80-9a49-3af57d5f430f",
   "metadata": {},
   "source": [
    "## Rouge\n",
    "\n",
    "> Recall-Oriented Understudy for Gisting Evaluation measures overlaps in n-grams between response and ground truth.\n",
    "\n",
    "RougeScoreEvaluator computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, a set of metrics used to evaluate automatic summarization and machine translation. It measures the overlap between generated text and reference summaries. ROUGE focuses on recall-oriented measures to assess how well the generated text covers the reference text. The ROUGE score is composed of precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23c6aa95-3746-4ea9-b4fc-57489e9a29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_evaluator = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_L, precision_threshold=0.6, recall_threshold=0.5, f1_score_threshold=0.55) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1123c495-f744-4242-8c31-fdd08065b5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_precision': 0.46153846153846156,\n",
       " 'rouge_recall': 1.0,\n",
       " 'rouge_f1_score': 0.631578947368421,\n",
       " 'rouge_precision_result': 'fail',\n",
       " 'rouge_recall_result': 'pass',\n",
       " 'rouge_f1_score_result': 'pass',\n",
       " 'rouge_precision_threshold': 0.6,\n",
       " 'rouge_recall_threshold': 0.5,\n",
       " 'rouge_f1_score_threshold': 0.55}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_evaluator(\n",
    "    response=\n",
    "    \"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6abe4-7f89-4eb4-9140-d4aa0372b35c",
   "metadata": {},
   "source": [
    "## Meteor\n",
    "\n",
    "> Metric for Evaluation of Translation with Explicit Ordering measures overlaps in n-grams between response and ground truth.\n",
    "\n",
    "MeteorScoreEvaluator measures the similarity by shared n-grams between the generated text and the ground truth, similar to the BLEU score, focusing on precision and recall. But it addresses limitations of other metrics like the BLEU score by considering synonyms, stemming, and paraphrasing for content alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e952564-1179-4f94-984a-cb20d43bfc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_evaluator = MeteorScoreEvaluator(threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "839c2628-d904-4742-934a-3b9d42ef8377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5831325301204818\n"
     ]
    }
   ],
   "source": [
    "answer = meteor_evaluator(\n",
    "    response=\"Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie first moments were in Warsaw.\")\n",
    "\n",
    "print(answer[\"meteor_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658b911-328d-4e92-955c-9fb2766b1cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
